
---

# ğŸš€ Kubernetes Horizontal Pod Autoscaler (HPA)

This project demonstrates how to use **Horizontal Pod Autoscaler (HPA)** in Kubernetes to **scale an NGINX deployment automatically** based on CPU usage.

HPA ensures your application can handle **high traffic loads dynamically** without manual intervention ğŸ’¡.

---

## ğŸ“– Real-Life Example

Imagine you run an **e-commerce website** ğŸ›’:

* During **normal hours**, you may only need **2-3 NGINX pods** to handle requests.
* But during a **flash sale** âš¡ or **festival season** ğŸ‰, traffic suddenly spikes ğŸš¦.
* Without autoscaling, your app could **crash** ğŸ˜¢ due to overload.
* With **HPA**, Kubernetes will automatically **increase pods** (e.g., from 2 â 10) to handle traffic.
* When traffic reduces, it will **scale down** again, saving ğŸ’° on resources.

---

## ğŸ› ï¸ Prerequisites

Make sure you have:

* âœ… Kubernetes cluster (minikube, kind, EKS, GKE, AKS, etc.)
* âœ… `kubectl` installed & configured
* âœ… Metrics server installed (HPA needs metrics)

ğŸ‘‰ Install metrics server if not already:

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```


## ğŸ“œ Deployment YAML

### ğŸ”¹ NGINX Deployment & Service (`nginx-deployment.yaml`)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: "100m"
          limits:
            cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
  - port: 80
    targetPort: 80
```

---

### ğŸ”¹ HPA YAML (`hpa.yaml`)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
```

âš¡ This means:

* Minimum **2 pods**
* Maximum **10 pods**
* Scale up when CPU usage > **50%**

---

## ğŸš€ Steps to Deploy

1ï¸âƒ£ Apply the NGINX deployment & service:

```bash
kubectl apply -f nginx-deployment.yaml
```

2ï¸âƒ£ Apply the HPA:

```bash
kubectl apply -f hpa.yaml
```

3ï¸âƒ£ Check HPA status:

```bash
kubectl get hpa
```

---

## ğŸ”¥ Stress Testing the Autoscaler

We will use **Apache Benchmark (`ab`)** to simulate traffic:

```bash
kubectl run -i --tty load-generator --image=busybox /bin/sh
```

Inside the pod, run:

```bash
while true; do wget -q -O- http://nginx-service; done
```

This creates continuous load â†’ CPU usage increases â†’ HPA scales pods ğŸš€

---

## ğŸ“Š Verify Scaling

Check pods scaling in action:

```bash
kubectl get pods -w
```

You will see pods scaling up when traffic increases âš¡ and scaling down when load reduces â¬‡ï¸.

---

## ğŸ¯ Outcome

âœ… Automatic pod scaling
âœ… High availability during traffic spikes
âœ… Cost efficiency by scaling down when not needed

---


